{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import ssl\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "_create_unverified_https_context = ssl._create_unverified_context\n",
    "\n",
    "ssl._create_default_https_context = _create_unverified_https_context\n",
    "nltk.download('punkt', download_dir=str(ROOT))\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "with open('HDFS_2000.log', 'r') as file:\n",
    "    original_text = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def absolute_to_relative_path(text):\n",
    "    text = re.sub(r\"/user/root/rand/\", \"/\", text)\n",
    "    return text\n",
    "\n",
    "def remplace_common(text):\n",
    "    tokenize_text = word_tokenize(text)\n",
    "\n",
    "    common_dict = {\n",
    "        'CLUSTER_SYSTEM_NUMBER': r\"\\d+$\",\n",
    "        'CLUSTER_SYSTEM_SV_NUMBER': r\"\\d+'[bodhBODH][\\da-fA-Fxz]+$\",\n",
    "        'CLUSTER_SYSTEM_HEX_VAL': r\"0x[\\da-fA-F]+$\",\n",
    "        \"CLUSTER_SYSTEM_TIME\": r\"\\d+(.\\d+){0,1}(s|ms|us|ps|fs)$\",\n",
    "        \"CLUSTER_SYSTEM_IP_PORT\": r\"/(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}):(\\d{1,5})\",\n",
    "        \"CLUSTER_SYSTEM_IP\": r\"/(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})\",\n",
    "    }\n",
    "    for i, word in enumerate(tokenize_text):\n",
    "        for remplace, regex in common_dict.items():\n",
    "            match = re.search(regex, word)\n",
    "            if match:\n",
    "                tokenize_text[i] = remplace\n",
    "\n",
    "    detokenizer = TreebankWordDetokenizer()\n",
    "    detokenizer_text = detokenizer.detokenize(tokenize_text)\n",
    "    return detokenizer_text\n",
    "\n",
    "def clean_data(text):\n",
    "    text = absolute_to_relative_path(text)\n",
    "    text = remplace_common(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "process_text = list(map(clean_data, original_text))\n",
    "process_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('google-bert/bert-base-cased')\n",
    "model = BertModel.from_pretrained('google-bert/bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# error_list = error_list[:100]\n",
    "inputs = tokenizer(process_text, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, )\n",
    "\n",
    "embeddings = outputs.last_hidden_state\n",
    "cls_embeddings = embeddings[:, 0, :]\n",
    "cls_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import numpy as np\n",
    "\n",
    "cosine_distance_matrix = cosine_distances(cls_embeddings)\n",
    "cosine_distance_matrix = cosine_distance_matrix.astype(np.float64)\n",
    "\n",
    "clustering = hdbscan.HDBSCAN(min_cluster_size=3, metric='precomputed').fit(cosine_distance_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "labels = clustering.labels_\n",
    "\n",
    "clusters = {}\n",
    "\n",
    "for label, log_entry in zip(labels, original_text):\n",
    "    if label not in clusters:\n",
    "        clusters[label] = []\n",
    "    clusters[label].append(log_entry)\n",
    "    \n",
    "for cluster_lit in clusters.values():\n",
    "    cluster_lit.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for cluster in clusters.values():\n",
    "    print(\"-\"*20)\n",
    "    print(\"\\n\".join(cluster))\n",
    "    print(\"-\"*20)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
